
import os
import json
import re
import subprocess
from bs4 import BeautifulSoup
from pathlib import Path

def extract_chm_with_7zip(chm_file, output_dir="chm_output"):
    """ä½¿ç”¨7zipæå–CHMæ–‡ä»¶å¹¶å¤„ç†"""
    
    chm_path = Path(chm_file)
    if not chm_path.exists():
        print(f"âŒ CHMæ–‡ä»¶ä¸å­˜åœ¨: {chm_file}")
        return
    
    # æ£€æŸ¥7zipæ˜¯å¦å®‰è£…
    try:
        subprocess.run(['7z'], capture_output=True, check=False)
    except FileNotFoundError:
        print("âŒ è¯·å…ˆå®‰è£…7zip: brew install p7zip")
        return
    
    # åˆ›å»ºä¸´æ—¶æå–ç›®å½•
    temp_dir = "temp_chm_extract"
    os.makedirs(temp_dir, exist_ok=True)
    
    print(f"ğŸ“¦ æå–CHMæ–‡ä»¶: {chm_file}")
    
    # ä½¿ç”¨7zipæå–
    try:
        result = subprocess.run([
            '7z', 'x', str(chm_path), f'-o{temp_dir}', '-y'
        ], capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"âŒ 7zipæå–å¤±è´¥: {result.stderr}")
            return
            
    except Exception as e:
        print(f"âŒ æå–è¿‡ç¨‹å‡ºé”™: {e}")
        return
    
    print("âœ… CHMæ–‡ä»¶æå–æˆåŠŸ")
    
    # å¤„ç†HTMLæ–‡ä»¶
    documents, text = process_html_files(temp_dir, output_dir)
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•
    import shutil
    shutil.rmtree(temp_dir, ignore_errors=True)
    
    return documents

def process_html_files(extracted_dir, output_dir):
    """å¤„ç†æå–çš„HTMLæ–‡ä»¶"""
    
    os.makedirs(output_dir, exist_ok=True)
    documents = []
    check = []
    # æŸ¥æ‰¾HTMLæ–‡ä»¶
    html_files = []
    for ext in ['*.html', '*.htm']:
        html_files.extend(Path(extracted_dir).rglob(ext))
    
    print(f"ğŸ” æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    
    for i, html_file in enumerate(html_files, 1):
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç 
            content = None
            for encoding in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                try:
                    with open(html_file, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if not content:
                print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶: {html_file}")
                continue
            
            # è§£æHTML
            soup = BeautifulSoup(content, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
            for tag in soup(['script', 'style', 'meta', 'link']):
                tag.decompose()
            
            # æå–æ ‡é¢˜
            title = "\n"
            if soup.title and soup.title.get_text().strip():
                title = soup.title.get_text().strip()
            elif soup.find(['h1', 'h2', 'h3']):
                title = soup.find(['h1', 'h2', 'h3']).get_text().strip()
            
            # æå–æ–‡æœ¬
            text = soup.get_text()
            # æ¸…ç†æ–‡æœ¬ï¼Œä¿æŒæ®µè½ç»“æ„
            lines = [line.strip() for line in text.splitlines() if line.strip()]
            clean_text = '\n'.join(lines)
# åªä¿å­˜æœ‰æ„ä¹‰çš„å†…å®¹
            doc = {"content": clean_text}
            documents.append(doc)
            check.append(title)
            check.append(clean_text)

            print(f"ğŸ“„ å·²å¤„ç† {i}/{len(html_files)} ä¸ªæ–‡ä»¶")
        
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ–‡ä»¶å‡ºé”™ {html_file}: {e}")
    
    print(f"âœ… æˆåŠŸå¤„ç† {len(documents)} ä¸ªæ–‡æ¡£")
    
    # ä¿å­˜ç»“æœ
    # save_text_results(documents, output_dir)
    return documents, check

def save_text_results(documents, output_dir):
    """ä¿å­˜çº¯æ–‡æœ¬ç»“æœ"""
    
    output_path = Path(output_dir)
    
    # 1. JSONæ ¼å¼ (ä¿ç•™ç»“æ„åŒ–ä¿¡æ¯)
    json_file = output_path / "documents.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(documents, f, ensure_ascii=False, indent=2)
    
    # 2. å•ç‹¬çš„æ–‡æœ¬æ–‡ä»¶ (ä¾¿äºåç»­å¤„ç†)
    texts_dir = output_path / "texts"
    texts_dir.mkdir(exist_ok=True)
    
    for doc in documents:
        # å®‰å…¨çš„æ–‡ä»¶å
        safe_name = re.sub(r'[^\w\-_.]', '_', doc['title'])[:50]
        txt_file = texts_dir / f"{doc['id']}_{safe_name}.txt"
        
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(doc['content'])  # åªå†™å…¥çº¯æ–‡æœ¬ï¼Œä¸åŠ é¢å¤–ä¿¡æ¯
    
    # 3. å¸¦å…ƒæ•°æ®çš„æ–‡æœ¬æ–‡ä»¶ (å¦‚æœéœ€è¦ä¿ç•™æ¥æºä¿¡æ¯)
    texts_with_meta_dir = output_path / "texts_with_metadata"
    texts_with_meta_dir.mkdir(exist_ok=True)
    
    for doc in documents:
        safe_name = re.sub(r'[^\w\-_.]', '_', doc['title'])[:50]
        txt_file = texts_with_meta_dir / f"{doc['id']}_{safe_name}.txt"
        
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(f"æ ‡é¢˜: {doc['title']}\n")
            f.write(f"æ¥æºæ–‡ä»¶: {doc['metadata']['source_file']}\n")
            f.write(f"å­—æ•°: {doc['metadata']['word_count']}\n")
            f.write(f"è¡Œæ•°: {doc['metadata']['line_count']}\n")
            f.write("=" * 60 + "\n\n")
            f.write(doc['content'])
    
    # 4. æ‰€æœ‰æ–‡æœ¬åˆå¹¶ (é€‚åˆæ‰¹é‡å¤„ç†)
    combined_file = output_path / "all_text_combined.txt"
    with open(combined_file, 'w', encoding='utf-8') as f:
        for i, doc in enumerate(documents, 1):
            if i > 1:  # æ–‡æ¡£é—´æ·»åŠ åˆ†éš”ç¬¦
                f.write(f"\n\n{'='*80}\n")
                f.write(f"æ–‡æ¡£ {i}: {doc['title']}\n")
                f.write(f"æ¥æº: {doc['metadata']['source_file']}\n")
                f.write('='*80 + "\n\n")
            else:
                f.write(f"æ–‡æ¡£ {i}: {doc['title']}\n")
                f.write(f"æ¥æº: {doc['metadata']['source_file']}\n")
                f.write('='*80 + "\n\n")
            
            f.write(doc['content'])
    
    # 5. çº¯æ–‡æœ¬åˆå¹¶ (å®Œå…¨å¹²å‡€çš„æ–‡æœ¬)
    pure_text_file = output_path / "pure_text_combined.txt"
    with open(pure_text_file, 'w', encoding='utf-8') as f:
        for i, doc in enumerate(documents):
            if i > 0:
                f.write('\n\n')  # æ–‡æ¡£é—´åªç”¨ç©ºè¡Œåˆ†éš”
            f.write(doc['content'])
    
    # 6. æ–‡æ¡£ç´¢å¼• (ä¾¿äºå®šä½)
    index_file = output_path / "document_index.txt"
    with open(index_file, 'w', encoding='utf-8') as f:
        f.write("æ–‡æ¡£ç´¢å¼•\n")
        f.write("="*50 + "\n\n")
        for doc in documents:
            f.write(f"ID: {doc['id']}\n")
            f.write(f"æ ‡é¢˜: {doc['title']}\n")
            f.write(f"æ¥æº: {doc['metadata']['source_file']}\n")
            f.write(f"å­—æ•°: {doc['metadata']['word_count']:,}\n")
            f.write(f"è¡Œæ•°: {doc['metadata']['line_count']}\n")
            f.write("-" * 30 + "\n\n")
    
    # 7. ç»Ÿè®¡ä¿¡æ¯
    total_words = sum(doc['metadata']['word_count'] for doc in documents)
    total_chars = sum(doc['metadata']['char_count'] for doc in documents)
    
    stats = {
        "extraction_summary": {
            "total_documents": len(documents),
            "total_words": total_words,
            "total_characters": total_chars,
            "average_words_per_doc": total_words // len(documents) if documents else 0,
            "largest_doc": max(documents, key=lambda x: x['metadata']['word_count']) if documents else None,
            "smallest_doc": min(documents, key=lambda x: x['metadata']['word_count']) if documents else None
        },
        "output_files": {
            "structured_data": "documents.json",
            "individual_texts": "texts/*.txt",
            "texts_with_metadata": "texts_with_metadata/*.txt", 
            "combined_with_headers": "all_text_combined.txt",
            "pure_text_only": "pure_text_combined.txt",
            "document_index": "document_index.txt"
        }
    }
    
    with open(output_path / "extraction_stats.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æå–ç»Ÿè®¡:")
    print(f"   ğŸ“„ æ–‡æ¡£æ€»æ•°: {len(documents)}")
    print(f"   ğŸ“ æ€»å­—æ•°: {total_words:,}")
    print(f"   ğŸ“ æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"   ğŸ“ˆ å¹³å‡æ¯æ–‡æ¡£å­—æ•°: {stats['extraction_summary']['average_words_per_doc']:,}")
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   ğŸ“‹ documents.json - ç»“æ„åŒ–æ•°æ®")
    print(f"   ğŸ“‚ texts/ - çº¯æ–‡æœ¬æ–‡ä»¶ (æ— å…ƒæ•°æ®)")
    print(f"   ğŸ“‚ texts_with_metadata/ - å¸¦å…ƒæ•°æ®çš„æ–‡æœ¬æ–‡ä»¶")
    print(f"   ğŸ“„ all_text_combined.txt - åˆå¹¶æ–‡æœ¬ (å¸¦æ ‡é¢˜)")
    print(f"   ğŸ“„ pure_text_combined.txt - çº¯æ–‡æœ¬åˆå¹¶ (æ— æ ‡é¢˜)")
    print(f"   ğŸ“‹ document_index.txt - æ–‡æ¡£ç´¢å¼•")
    print(f"   ğŸ“Š extraction_stats.json - æå–ç»Ÿè®¡")
    
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print(f"   - ç”¨äºRAG: ä½¿ç”¨ texts/ ç›®å½•ä¸­çš„æ–‡ä»¶")
    print(f"   - æ‰¹é‡å¤„ç†: ä½¿ç”¨ pure_text_combined.txt")
    print(f"   - ä¿ç•™æ¥æº: ä½¿ç”¨ texts_with_metadata/ ç›®å½•")

# ä¸»å‡½æ•°
if __name__ == "__main__":
    import sys
    
    chm_file = "/Users/janice.xu/Downloads/COC6thç©å®¶æ•´åˆæ‰‹å†Œv1.7 Super Extra II.chm" 
    output_dir = f"{Path(chm_file).stem}_extracted"
    
    print("ğŸš€ å¼€å§‹å¤„ç†CHMæ–‡ä»¶...")
    documents = extract_chm_with_7zip(chm_file, output_dir)
    
    if documents:
        print(f"\nâœ… å¤„ç†å®Œæˆï¼è¾“å‡ºç›®å½•: {output_dir}")
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - documents.json (ç»“æ„åŒ–æ•°æ®)")
        print(f"   - texts/ (å•ç‹¬æ–‡æœ¬æ–‡ä»¶)")
        print(f"   - combined_text.txt (åˆå¹¶æ–‡æœ¬)")
        print(f"   - rag_chunks.json (RAGåˆ†å—)")
        print(f"   - statistics.json (ç»Ÿè®¡ä¿¡æ¯)")
    else:
        print("âŒ å¤„ç†å¤±è´¥")